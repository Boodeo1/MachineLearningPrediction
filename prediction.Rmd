---
title: "Prediction Assignment"
author: "Boodeo Nem"
date: "January 8, 2019"
output: html_document
keep_md: True 
---

#### Introduction  
In this project, data from accelerometers on the belt, forearm, arm, and dumbell of 6 male participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways have been used to predict how "well" the exercise was performed. The predictions were based on different models (decision trees, linear discriminants and random forest) and the model giving the highest accuracy is the one built under random forest. Therefore, this model is proposed for making predictions based on the raw test sets.   

#### Downloading the data  
Both the training and testing data have been downloaded through the respective links. The training set has been used to build the model while the testing set was used to test the predictions by the models.   

```{r}
library(caret); library(randomForest); library(rpart); library(rpart.plot)

if(!file.exists('data')) dir.create ('data')
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl1, destfile = "./data/predtraining.csv")
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl2, destfile = "./data/predtest.csv")
trainset <- read.csv("C:/Users/boodeo1/Desktop/Coursera/data/data/predtraining.csv", na.strings = c("NA", "#DIV/0!", ""))
testset <- read.csv("C:/Users/boodeo1/Desktop/Coursera/data/data/predtest.csv", header = TRUE, sep = "")
``` 
After a quick observation of the data, all missing values and variables which are not relevant for the study are removed.  

#### Cleaning the data  
```{r}
# Deleting all missing values
set.seed(456)
trainset <- trainset[,colSums(is.na(trainset))==0]
testset <- testset[,colSums(is.na(testset))==0]

# Removing irrelevant columns - these are columns 1 to 7 (i.e, X to num_window)
trainset <- trainset[,-c(1:7)]
testset <- testset[, -c(1:7)]
```

#### Splitting the data between training and testing  
The data is split between the training and testing set in the ratio (75:25).    
```{r}
set.seed(5642)
inTrain <- createDataPartition(y=trainset$classe, p=0.75, list = FALSE)
training <- trainset[inTrain, ]
testing <- trainset[-inTrain, ]
```

#### 10-fold Cross Validation  
Some variables in the training dataset appear to be highly correlated (more than 0.8). Thus, in order to avoid bias and high variances, a 10-fold cross validation is used.  

```{r}
tc <- trainControl(method = "cv", number = 10, verboseIter = FALSE, preProcOptions = "pca", allowParallel = TRUE)
```

#### Model 1: Predicting with decision trees  
The first model is built on decision trees, as this method iteratively splits the variables into groups and evalutates the homogeneity within each group. The split is carried out again if necessary.  
```{r}
set.seed(2826)
library(rattle)
model1 <- train(classe ~.,data=training, method = "rpart", trControl = tc)
pred1 <- predict(model1, testing, type = "raw")
confusionMatrix(pred1, testing$classe)
fancyRpartPlot(model1$finalModel)
```
  
  Based on the confusion matrix, the decision tree method seems not fully capable of making accurate predictions. The out of sample error is quite high at 51%, resulting in low prediction accuracy.   

#### Model 2: Linear discriminant analysis   
```{r}
set.seed(1234)
modlda <- train(classe ~.,data = training, method = "lda", trControl = tc)
plda <- predict(modlda, testing)
confusionMatrix(plda, testing$classe)
```
The prediction accuracy of this model is slightly better suggesting that linear discriminant analysis does a better prediction than decision trees. The out of sample error is also comparatively better at nearly 32%.  

#### Comparing model 1 to model 2  
```{r}
equalPredictions <- (pred1 == plda)
qplot(classe, color = equalPredictions, data = testing)
```
  
  The graph above shows that these two methods produce quite diverging predictions about the outcome. It is therefore worth investigating further with a different model.   

#### Model 3: Predicting with Random Forests  
```{r}
set.seed(8976)
model3 <- randomForest(classe ~.,data = training, method = "class", trControl = tc)
predrf <- predict(model3, testing, type = "class")
confusionMatrix(predrf, testing$classe)
```
The confusion matrix shows that the random forest method yields a much better prediction as this method bootstraps the samples and considers multiple trees while making predictions. The out of sample error is significantly much lower compared to the previous two models, i.e. 1.06%. This means that the predictions generated by the random forest method is more reliable than the others. A plot for the new data predictions is also produced in the appendix.  

#### Appendix  
Predicting new values  
```{r}
testing$predRight <- predrf==testing$classe
qplot(classe, color = predRight, data = testing, main = "New Data Predictions")
```




